{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the matplotlib plotted graph within notebook lines.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case that your tqdm is not installed. Please go to the Start(Windows)->Anaconda2(64-bit)->Anaconda Prompt(py35)\n",
    "\n",
    "### type: conda install tqdm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "# pandas:Data framework library for Python\n",
    "# sklearn: Library to perform machine learning tasks\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import re\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn \n",
    "import sklearn.datasets\n",
    "import sklearn.metrics as metrics \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract data\n",
    "#!gzip -d -k 20news-19997.tar.gz\n",
    "#!tar -xf 20news-19997.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "# display newsgroups directories\n",
    "!ls 20_newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFilesDirectory(datapath='20_newsgroups'):\n",
    "    # create file directory for all files\n",
    "    files = []\n",
    "    for (path, dirnames, filenames) in os.walk(datapath):\n",
    "        files.extend(os.path.join(path, name) for name in filenames)\n",
    "    # putting file directories into pandas dataframe\n",
    "    directorydf= pd.DataFrame(files)\n",
    "    directorydf.columns = ['Directories']\n",
    "    \n",
    "    return directorydf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorydf = getFilesDirectory().head(n = 5000)\n",
    "#directorydf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create function to help encoding the targets\n",
    "def label_target (d):\n",
    "    if d.find(\"alt.atheism\") > 0 :\n",
    "      return 0\n",
    "    if d.find(\"comp.graphics\") > 0:\n",
    "      return 1\n",
    "    if d.find(\"comp.os.ms-windows.misc\") > 0:\n",
    "      return 2\n",
    "    if d.find(\"comp.sys.ibm.pc.hardware\") > 0:\n",
    "      return 3\n",
    "    if d.find(\"comp.sys.mac.hardware\") > 0:\n",
    "      return 4\n",
    "    if d.find(\"comp.windows.x\") > 0:\n",
    "      return 5\n",
    "    if d.find(\"misc.forsale\") > 0:\n",
    "      return 6\n",
    "    if d.find(\"rec.autos\") > 0:\n",
    "      return 7\n",
    "    if d.find(\"rec.motorcycles\") > 0:\n",
    "      return 8\n",
    "    if d.find(\"rec.sport.baseball\") > 0:\n",
    "      return 9\n",
    "    if d.find(\"rec.sport.hockey\") > 0:\n",
    "      return 10\n",
    "    if d.find(\"sci.crypt\") > 0:\n",
    "      return 11\n",
    "    if d.find(\"sci.electronics\") > 0:\n",
    "      return 12\n",
    "    if d.find(\"sci.med\") > 0:\n",
    "      return 13\n",
    "    if d.find(\"sci.space\") > 0:\n",
    "      return 14\n",
    "    if d.find(\"soc.religion.christian\") > 0:\n",
    "      return 15\n",
    "    if d.find(\"talk.politics.guns\") > 0:\n",
    "      return 16\n",
    "    if d.find(\"talk.politics.mideast\") > 0:\n",
    "      return 17\n",
    "    if d.find(\"talk.politics.misc\") > 0:\n",
    "      return 18\n",
    "    if d.find(\"talk.religion.misc\") > 0:\n",
    "      return 19\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "i) The feature encoding method: stem, to exclude stop words\n",
    "ii) The method for ranking features: MAP\n",
    "iii) Number of features to select: 500, as we have 20000 documents, select a relative big number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataPreprocessor(k, fs, encode, directorydf, size=None):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        k: int. number of features to use\n",
    "        fs: string. From ['tf', 'mi']\n",
    "        encode: string. From ['tf', 'boolean']\n",
    "        drectorydf: Dataframe. It is given, see above support functions\n",
    "        size: int. Sample size. Default should be the data size.\n",
    "    OUTPUT\n",
    "        data: Dataframe. preprocessed data\n",
    "    \n",
    "    ps: 'tf' means term frequency, 'mi' means mutual information\n",
    "    \"\"\"\n",
    "\n",
    "    ########### your code goes here ###########\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "    from whoosh.analysis import Filter\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    import codecs\n",
    "    import numpy\n",
    "    counter = Counter()\n",
    "    \n",
    "    wnLemm = WordNetLemmatizer()\n",
    "    # Open the files and count the word frequency in each file in a loop and update the counter after finished processing a file\n",
    "    for rownum, row in enumerate(directorydf.itertuples()):\n",
    "        with codecs.open(row.Directories,\"r\" ,encoding='utf-8', errors='ignore') as myfile:\n",
    "            counter.update([word for word in re.findall(r'\\w+', wnLemm.lemmatize(myfile.read(), 'v'))])\n",
    "        if (rownum % 1000 == 0):\n",
    "            print(\"processed %d files\" % (rownum+1))\n",
    "    # filter words\n",
    "    data = []\n",
    "    feature_target = []\n",
    "    topk = []\n",
    "    if fs == 'tf':     \n",
    "    # find top k words occured most requent in the document\n",
    "        topk = counter.most_common(k)\n",
    "    \n",
    "    # find top k words with highest mi scores in the document\n",
    "    elif fs =='mi':\n",
    "        count = 0\n",
    "        for key in dict(counter):\n",
    "            count = count + 1\n",
    "        words= counter.most_common(count)\n",
    "        \n",
    "        tempCounter = Counter()\n",
    "        for rownum, row in enumerate(directorydf.itertuples()):\n",
    "            with codecs.open(row.Directories,\"r\" ,encoding='utf-8', errors='ignore') as myfile:\n",
    "                tempCounter= Counter([word for word in re.findall(r'\\w+',wnLemm.lemmatize(myfile.read(), 'v'))])\n",
    "                frequency = [tempCounter[word] if tempCounter[word] > 0 else 0 for (word,wordCount) in words] \n",
    "                feature_target.append(frequency+[label_target(row.Directories)])\n",
    "    \n",
    "        features = pd.DataFrame(feature_target)\n",
    "        dfName = []\n",
    "        for c in words:\n",
    "            dfName.append(c[0])\n",
    "        \n",
    "        features.columns = dfName +['myTarget']\n",
    "        features_df = features[dfName]\n",
    "        feature = features_df.as_matrix()\n",
    "        target_df = features['myTarget']\n",
    "        target = target_df.as_matrix()\n",
    "        mi = mutual_info_classif(feature, target)\n",
    "        arr = numpy.array(mi)\n",
    "        ind = arr.argsort()[-k:][::-1]\n",
    "        for i in ind:\n",
    "            topk.append((words[i][0],words[i][1]))\n",
    "    \n",
    "    np = []\n",
    "    # now we had top k words, count the frequecy (binary) of these words in individual file\n",
    "    for rownum, row in enumerate(directorydf.itertuples()):\n",
    "        with codecs.open(row.Directories,\"r\" ,encoding='utf-8', errors='ignore') as myfile:\n",
    "            tempCounter = Counter([word for word in re.findall(r'\\w+',wnLemm.lemmatize(myfile.read(), 'v'))])\n",
    "            if encode == 'boolean':\n",
    "            # if the word appears in the doc, then 1, else \n",
    "                topkinDoc = [1 if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n",
    "            elif encode == 'tf':\n",
    "                topkinDoc = [tempCounter[word] if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n",
    "        # create a list for top k words with encoded target and its label\n",
    "            np.append(topkinDoc+[label_target(row.Directories)])\n",
    "        if (rownum % 1000 == 0):\n",
    "            print(\"processed %d files\" % (rownum+1))\n",
    "     \n",
    "    data = pd.DataFrame(np)\n",
    "    dfName = []\n",
    "    for c in topk:\n",
    "        dfName.append(c[0])\n",
    "    data.columns = dfName +['myTarget']\n",
    "\n",
    "    \n",
    "    ###########         end         ###########%%\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1 files\n",
      "processed 1001 files\n",
      "processed 2001 files\n",
      "processed 3001 files\n",
      "processed 4001 files\n",
      "processed 1 files\n",
      "processed 1001 files\n",
      "processed 2001 files\n",
      "processed 3001 files\n",
      "processed 4001 files\n"
     ]
    }
   ],
   "source": [
    "data = dataPreprocessor(k=500, fs='tf', encode='tf', directorydf=directorydf, size=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Confidence Interval Function\n",
    "import scipy.stats\n",
    "from math import sqrt\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0*np.array(data)\n",
    "    n = len(a)\n",
    "    mu,sd = np.mean(a),np.std(a)\n",
    "    z = stats.t.ppf(confidence, n)\n",
    "    h=z*sd/sqrt(n)\n",
    "    return mu, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomSplitCI(data, clf, num_run, **params):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        data: 2D numpy array. Pre-processed data\n",
    "        clf: string. Name of the classifier from ['LR', 'SVM', 'NB']\n",
    "        num_run: int. How many times you want to run for random evaluation?\n",
    "        params: string->real. Hyper-parameter of classifier. PS: c=1.0, r=0.01\n",
    "    \n",
    "    OUTPUT\n",
    "        train_scores: list. Results of trails\n",
    "        test_scores: list. Results of trails\n",
    "        train_mean: scalar. Average accuracy\n",
    "        test_mean: scalar. Average accuracy\n",
    "        train_ci: scalar. Confidence Interval\n",
    "        test_ci: scalar. Confidence Interval\n",
    "    \"\"\"\n",
    "    \n",
    "    ########### your code goes here ###########\n",
    "    import numpy as np\n",
    "    features_df= data.iloc[:,0:-1]\n",
    "    features = features_df.as_matrix()\n",
    "    target = data['myTarget'].as_matrix()\n",
    "    cms = {}\n",
    "    scores_tr = []\n",
    "    scores_ta = []\n",
    "    cv = StratifiedKFold(target, n_folds=10)\n",
    "    print(cv)\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        \n",
    "        # train then immediately predict the test set\n",
    "        #clfModel = clf.fit(features[train], target[train])\n",
    "        #target_pred = clfModel.predict(features[test])\n",
    "    \n",
    "        if clf == 'LR':\n",
    "            clf = LogisticRegression()\n",
    "\n",
    "        # separate datasets into training and test datasets once, no folding\n",
    "            #features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3)\n",
    "        # train the features and target datasets and fit to a model\n",
    "            clfModel = clf.fit(features[train], target[train])\n",
    "        # predict target with feature test set using trained model\n",
    "            target_pred = clfModel.predict(features[test])\n",
    "        \n",
    "\n",
    "    \n",
    "        elif clf == 'SVM':\n",
    "        # randomize and separate datasets into training and test datasets once, no folding\n",
    "            features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2)\n",
    "            parameters = [{'kernel': ['linear'], 'C': [1]}]\n",
    "\n",
    "        # find the best parameters to optimize accuracy\n",
    "            svc_clf = SVC(C=1, probability= True)\n",
    "            clfSVM = GridSearchCV(svc_clf, parameters, cv=10, scoring='accuracy') #5 folds\n",
    "            clfSVM.fit(features_train, target_train) #train the model \n",
    "            print(\"Best parameters found from SVM's:\")\n",
    "            print(clfSVM.best_params_)\n",
    "            print(\"Best score found from SVM's:\")    \n",
    "            print (clfSVM.best_score_)\n",
    "        \n",
    "        elif clf == 'NB':\n",
    "            clf = GaussianNB()\n",
    "            clfModel = clf.fit(features[train], target[train])\n",
    "            target_pred = clfModel.predict(features[test])\n",
    "            pd.DataFrame(metrics.confusion_matrix(target_test, target_predNB), columns=labels, index=labels)\n",
    "        \n",
    "        cms[i] = pd.DataFrame(metrics.confusion_matrix(target[test], target_pred), columns=target, index=target)\n",
    "        scores_ta.append(metrics.accuracy_score(target[test], target_pred))\n",
    "        #scores_tr.append(metrics.accuracy_score(target[train], features[test]))\n",
    "   \n",
    "    print(scores_ta)\n",
    "    train_scores = np.mean(scores_ta)\n",
    "    test_scores = [33]\n",
    "    train_mean = 1.0\n",
    "    test_mean = 1.0\n",
    "    train_ci = 1.0\n",
    "    test_ci = 1.0\n",
    "    print(\"The accuracy score for this training is\", test_scores)\n",
    "    \n",
    "    # compute the confusion matrix on each fold, convert it to a DataFrame and stash it for later compute\n",
    "        #cms[i] = pandas.DataFrame(metrics.confusion_matrix(target[test], target_pred), columns=labels, index=labels)\n",
    "    # stash the overall accuracy on the test set for the fold too\n",
    "        #scores.append(metrics.accuracy_score(target[test], target_pred))\n",
    "    ###########         end         ###########\n",
    "    return train_scores,test_scores,train_mean,test_mean,train_ci,test_ci\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.cross_validation.StratifiedKFold(labels=[0 0 0 ..., 4 4 4], n_folds=10, shuffle=False, random_state=None)\n",
      "[0.98399999999999999, 0.98399999999999999, 0.98399999999999999, 0.98399999999999999, 0.98399999999999999, 0.98399999999999999, 0.98399999999999999, 0.98399999999999999, 0.98399999999999999, 0.98399999999999999]\n",
      "The accuracy score for this training is [33]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.98399999999999999, [33], 1.0, 1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomSplitCI(data, 'LR', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores,test_scores,train_mean,test_mean,train_ci,test_ci = randomSplitCI(data, 'LR', 10, c=1.0)\n",
    "print(\"Train\\\n",
    "    \\nResult of trails:{0} \\\n",
    "    \\nAverage Accuracy: {1} \\\n",
    "    \\nConfidence Interval: {2}\\n\".format(train_scores, train_mean, train_ci)\n",
    "     )\n",
    "print(\"Test\\\n",
    "    \\nResult of trails:{0} \\\n",
    "    \\nAverage Accuracy: {1} \\\n",
    "    \\nConfidence Interval: {2}\".format(test_scores, test_mean, test_ci)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomSplitCM(data, clf, num_run, **params):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        data: Dataframe. Pre-processed data\n",
    "        clf: string. Name of the classifier from ['LR', 'SVM', 'NB']\n",
    "        params: string->real. Hyper-parameter of classifier. PS: c=1.0, r=0.01\n",
    "    \n",
    "    OUTPUT\n",
    "        cm: pandas.DataFrame. Confusion Matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ########### your code goes here ###########\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########         end         ###########\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = randomSplitCM(data, 'LR', 10, c=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureSizeAC(data, clf, num_run, **params):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        data: Dataframe. Pre-processed data\n",
    "        clf: string. Name of the classifier from ['LR', 'SVM', 'NB']\n",
    "        params: string->real. Hyper-parameter of classifier. PS: c=1.0, r=0.01\n",
    "    \n",
    "    OUTPUT\n",
    "        train_mean_fs: list.\n",
    "        train_ci_fs: list.\n",
    "        test_mean_fs: list.\n",
    "        test_ci_fs: list.\n",
    "        \n",
    "    \"\"\"\n",
    "    feature_precentage = np.linspace(0.1, 1, 10, endpoint=True)\n",
    "    \n",
    "    ########### your code goes here ###########\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########         end         ###########\n",
    "    return train_mean_fs, train_ci_fs, test_mean_fs, test_ci_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mean_fs, train_ci_fs, test_mean_fs, test_ci_fs = featureSizeAC(data, 'LR', 10, c=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureSizePlot(feature_size, train_mean_fs, train_ci_fs, test_mean_fs, test_ci_fs):\n",
    "    # First illustrate basic pyplot interface, using defaults where possible.\n",
    "    plt.figure()\n",
    "    test_curve=plt.errorbar(feature_size, test_mean_fs, color=sns.xkcd_rgb[\"pale red\"], yerr=test_ci_fs)\n",
    "    train_curve=plt.errorbar(feature_size, train_mean_fs,color=sns.xkcd_rgb[\"denim blue\"], yerr=train_ci_fs)\n",
    "    plt.legend([test_curve, train_curve], ['Test', 'Train'])\n",
    "    plt.xlabel('Feature Percentage')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"Accuracy vs Feature Size\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureSizePlot(np.linspace(0.1, 1, 10, endpoint=True),train_mean_fs, train_ci_fs, test_mean_fs, test_ci_fs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperParameterAC(data, clf, num_run):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        data: Dataframe. Pre-processed data\n",
    "        clf: string. Name of the classifier from ['LR', 'SVM', 'NB']\n",
    "    \n",
    "    OUTPUT\n",
    "        test_mean_hp: list.  mean accuracy list of test\n",
    "        test_ci_hp: list. confidence interval list of test\n",
    "        train_mean_hp: list. mean accuracy list of train\n",
    "        train_ci_hp: list. confidence interval list of train\n",
    "        \n",
    "    NOTE \n",
    "        randomSplitCI could be the sub-route of this function\n",
    "    \"\"\"\n",
    "    params = np.logspace(-4, 4, num=9)\n",
    "    \n",
    "    ########### your code goes here ###########\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########         end         ###########\n",
    "    return train_mean_hp, train_ci_hp, test_mean_hp, test_ci_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mean_hp, train_ci_hp, test_mean_hp, test_ci_hp = hyperParameterAC(data, 'LR', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperParameterPlot(params, train_mean_hp, train_ci_hp, test_mean_hp, test_ci_hp):\n",
    "    # First illustrate basic pyplot interface, using defaults where possible.\n",
    "    plt.figure()\n",
    "    test_curve=plt.errorbar(params, test_mean_hp, color=sns.xkcd_rgb[\"pale red\"], yerr=test_ci_hp)\n",
    "    train_curve=plt.errorbar(params, train_mean_hp,color=sns.xkcd_rgb[\"denim blue\"], yerr=train_ci_hp)\n",
    "    plt.legend([test_curve, train_curve], ['Test', 'Train'])\n",
    "    plt.xlabel('Parameter')\n",
    "    plt.xscale(\"log\")\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"Accuracy vs Parameters\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperParameterPlot(np.logspace(-4, 4, num=9),train_mean_hp, train_ci_hp, test_mean_hp, test_ci_hp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataSizeAC(data, clf, num_run):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        data: Dataframe. Pre-processed data\n",
    "        clf: string. Name of the classifier from ['LR', 'SVM', 'NB']\n",
    "        params: string->real. Hyper-parameter of classifier. PS: c=1.0, r=0.01\n",
    "    \n",
    "    OUTPUT\n",
    "        test_mean_ds: list.  mean accuracy list of test\n",
    "        test_ci_ds: list. confidence interval list of test\n",
    "        train_mean_ds: list. mean accuracy list of train\n",
    "        train_ci_ds: list. confidence interval list of train\n",
    "        \n",
    "        \n",
    "        \n",
    "    NOTE \n",
    "        randomSplitCI could be the sub-route of this function\n",
    "    \"\"\"\n",
    "    data_precentage = np.linspace(0.1, 1, 10, endpoint=True)\n",
    "    \n",
    "    ########### your code goes here ###########\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########         end         ###########\n",
    "    return train_mean_ds, train_ci_ds, test_mean_ds, test_ci_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mean_ds, train_ci_ds, test_mean_ds, test_ci_ds = dataSizeAC(data, 'LR', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataSizePlot(datasize, train_mean_ds, train_ci_ds, test_mean_ds, test_ci_ds):\n",
    "    # First illustrate basic pyplot interface, using defaults where possible.\n",
    "    plt.figure()\n",
    "    test_curve=plt.errorbar(datasize, test_mean_ds, color=sns.xkcd_rgb[\"pale red\"], yerr=test_ci_ds)\n",
    "    train_curve=plt.errorbar(datasize, train_mean_ds,color=sns.xkcd_rgb[\"denim blue\"], yerr=train_ci_ds)\n",
    "    plt.legend([test_curve, train_curve], ['Test', 'Train'])\n",
    "    plt.xlabel('Data Percentage')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"Accuracy vs Data Percentage\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataSizePlot(np.linspace(0.1, 1, 10, endpoint=True),train_mean_ds, train_ci_ds, test_mean_ds, test_ci_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to get this answer easily by running dataPreprocessor fuction and randomSplitCI function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
